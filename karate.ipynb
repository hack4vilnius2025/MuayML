{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "455fd45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries successful!\n",
      "OpenCV version: 4.12.0\n",
      "\n",
      "Loading YOLOv11 pose model...\n",
      "Model loaded successfully!\n",
      "\n",
      "Initializing ADVANCED knuckle-based wrist detection with OpenCV...\n",
      "âœ“ Knuckle detection system ready (contour analysis, orientation detection)!\n",
      "âœ“ INVERTED LOGIC: Knuckles UP = Wrist DOWN | Knuckles DOWN = Wrist UP!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Importing libraries successful!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "\n",
    "# Initialize YOLOv11 pose model\n",
    "print(\"\\nLoading YOLOv11 pose model...\")\n",
    "model = YOLO('yolo11n-pose.pt')  # Using YOLOv11 nano pose model\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize enhanced image analysis for hand position detection\n",
    "print(\"\\nInitializing ADVANCED knuckle-based wrist detection with OpenCV...\")\n",
    "print(\"âœ“ Knuckle detection system ready (contour analysis, orientation detection)!\")\n",
    "print(\"âœ“ INVERTED LOGIC: Knuckles UP = Wrist DOWN | Knuckles DOWN = Wrist UP!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f1f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found reference: Nika3.mp4\n",
      "âœ“ Found nikadata: NikaData.mp4\n",
      "âœ“ Found matas: Matas.mp4\n",
      "\n",
      "Keypoint mapping defined successfully!\n",
      "Reference: Nika3.mp4\n",
      "NikaData: NikaData.mp4\n",
      "Matas: Matas.mp4\n",
      "\n",
      "ðŸŽ¯ Will compare both NikaData and Matas against Nika3 (Perfect Reference)\n"
     ]
    }
   ],
   "source": [
    "# Define video paths\n",
    "VIDEO_DIR = Path(r\"c:\\Users\\sapok\\Documents\\GitHub\\MuayML\")\n",
    "\n",
    "videos = {\n",
    "    'reference': VIDEO_DIR / \"nika3_perfect_reference.mp4\",  # Perfect reference video\n",
    "    'input': VIDEO_DIR / \"videos.mp4\"                        # Your video to analyze\n",
    "}\n",
    "\n",
    "output_video = VIDEO_DIR / \"analyzed.mp4\"  # Output analyzed video\n",
    "\n",
    "# Verify videos exist\n",
    "for name, path in videos.items():\n",
    "    if path.exists():\n",
    "        print(f\"âœ“ Found {name}: {path.name}\")\n",
    "    else:\n",
    "        print(f\"âœ— Missing {name}: {path.name}\")\n",
    "\n",
    "# YOLO Pose keypoint indices (COCO format)\n",
    "# 0: nose, 1-2: eyes, 3-4: ears, 5: left shoulder, 6: right shoulder\n",
    "# 7: left elbow, 8: right elbow, 9: left wrist, 10: right wrist\n",
    "# 11: left hip, 12: right hip, 13: left knee, 14: right knee\n",
    "# 15: left ankle, 16: right ankle\n",
    "\n",
    "KEYPOINT_MAPPING = {\n",
    "    'head': 0,  # nose\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_hand': 9,  # wrist\n",
    "    'right_hand': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'chest': 5,  # approximating with left shoulder\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_toe': 15,  # ankle\n",
    "    'right_toe': 16\n",
    "}\n",
    "\n",
    "print(\"\\nKeypoint mapping defined successfully!\")\n",
    "print(f\"Input video: {videos['input'].name}\")\n",
    "print(f\"Reference: {videos['reference'].name}\")\n",
    "print(f\"Output: {output_video.name}\")\n",
    "print(\"\\nðŸŽ¯ Will compare your video against Perfect Reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fde338b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def extract_keypoints_from_frame(frame, model, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Extract keypoints from a single frame using YOLOv11 pose detection.\n",
    "    Returns dictionary of keypoint coordinates with confidence filtering.\n",
    "    \"\"\"\n",
    "    results = model(frame, verbose=False, conf=0.5)  # Increased confidence threshold\n",
    "    \n",
    "    if len(results) > 0 and results[0].keypoints is not None:\n",
    "        keypoints = results[0].keypoints.xy.cpu().numpy()\n",
    "        confidences = results[0].keypoints.conf.cpu().numpy()\n",
    "        \n",
    "        if len(keypoints) > 0 and len(confidences) > 0:\n",
    "            # Get first person detected\n",
    "            person_keypoints = keypoints[0]\n",
    "            person_confidences = confidences[0]\n",
    "            \n",
    "            # Extract relevant keypoints with confidence filtering\n",
    "            extracted = {}\n",
    "            for name, idx in KEYPOINT_MAPPING.items():\n",
    "                x, y = person_keypoints[idx]\n",
    "                conf = person_confidences[idx]\n",
    "                \n",
    "                # Only include keypoint if detected with sufficient confidence\n",
    "                if x > 0 and y > 0 and conf >= confidence_threshold:\n",
    "                    extracted[name] = (int(x), int(y), float(conf))\n",
    "                else:\n",
    "                    extracted[name] = None\n",
    "            \n",
    "            # Calculate chest center from both shoulders\n",
    "            left_shoulder = person_keypoints[5]  # Left shoulder\n",
    "            right_shoulder = person_keypoints[6]  # Right shoulder\n",
    "            left_shoulder_conf = person_confidences[5]\n",
    "            right_shoulder_conf = person_confidences[6]\n",
    "            \n",
    "            if (left_shoulder[0] > 0 and left_shoulder[1] > 0 and left_shoulder_conf >= confidence_threshold and\n",
    "                right_shoulder[0] > 0 and right_shoulder[1] > 0 and right_shoulder_conf >= confidence_threshold):\n",
    "                # Calculate center point between shoulders\n",
    "                chest_x = int((left_shoulder[0] + right_shoulder[0]) / 2)\n",
    "                chest_y = int((left_shoulder[1] + right_shoulder[1]) / 2)\n",
    "                chest_conf = (left_shoulder_conf + right_shoulder_conf) / 2\n",
    "                \n",
    "                # Store chest position with shoulder width info\n",
    "                shoulder_width = int(abs(right_shoulder[0] - left_shoulder[0]))\n",
    "                extracted['chest'] = (chest_x, chest_y, float(chest_conf), shoulder_width)\n",
    "                    \n",
    "            return extracted\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def draw_keypoint_boxes(frame, keypoints, color=(0, 255, 0), box_size=20):\n",
    "    \"\"\"\n",
    "    Draw precise boxes around detected keypoints with adaptive sizing.\n",
    "    color: (B, G, R) format - (0, 255, 0) for GREEN, (0, 0, 255) for RED\n",
    "    \"\"\"\n",
    "    # Define body-part specific box sizes for better precision\n",
    "    body_part_sizes = {\n",
    "        'head': 25,\n",
    "        'left_elbow': 18,\n",
    "        'right_elbow': 18,\n",
    "        'left_hand': 15,\n",
    "        'right_hand': 15,\n",
    "        'left_hip': 20,\n",
    "        'right_hip': 20,\n",
    "        'chest': 22,  # Will be overridden by actual shoulder width\n",
    "        'left_knee': 18,\n",
    "        'right_knee': 18,\n",
    "        'left_toe': 15,\n",
    "        'right_toe': 15\n",
    "    }\n",
    "    \n",
    "    for name, data in keypoints.items():\n",
    "        if data is not None:\n",
    "            x, y = data[0], data[1]\n",
    "            conf = data[2] if len(data) > 2 else 1.0\n",
    "            \n",
    "            # Special handling for chest - use shoulder width\n",
    "            if name == 'chest' and len(data) > 3:\n",
    "                shoulder_width = data[3]\n",
    "                half_width = shoulder_width // 2\n",
    "                half_height = 15  # Fixed height for chest box\n",
    "                \n",
    "                # Draw wider rectangle spanning chest\n",
    "                thickness = max(2, int(conf * 3))\n",
    "                cv2.rectangle(frame, \n",
    "                             (x - half_width, y - half_height), \n",
    "                             (x + half_width, y + half_height), \n",
    "                             color, thickness)\n",
    "                \n",
    "                # Add label\n",
    "                label = \"chest\"\n",
    "                label_y = y - half_height - 8\n",
    "                \n",
    "                # Add background for text readability\n",
    "                (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "                cv2.rectangle(frame, \n",
    "                             (x - half_width, label_y - text_height - 2),\n",
    "                             (x - half_width + text_width + 2, label_y + 2),\n",
    "                             color, -1)\n",
    "                \n",
    "                cv2.putText(frame, label, \n",
    "                           (x - half_width + 1, label_y),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 0), 1)\n",
    "            else:\n",
    "                # Standard box for other body parts\n",
    "                half_size = body_part_sizes.get(name, box_size) // 2\n",
    "                \n",
    "                # Draw rectangle with thickness based on confidence\n",
    "                thickness = max(2, int(conf * 3))\n",
    "                cv2.rectangle(frame, \n",
    "                             (x - half_size, y - half_size), \n",
    "                             (x + half_size, y + half_size), \n",
    "                             color, thickness)\n",
    "                \n",
    "                # Add label with confidence score\n",
    "                label = f\"{name.replace('_', ' ')}\"\n",
    "                \n",
    "                # Position label above the box\n",
    "                label_y = y - half_size - 8\n",
    "                \n",
    "                # Add background for text readability\n",
    "                (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "                cv2.rectangle(frame, \n",
    "                             (x - half_size, label_y - text_height - 2),\n",
    "                             (x - half_size + text_width + 2, label_y + 2),\n",
    "                             color, -1)\n",
    "                \n",
    "                cv2.putText(frame, label, \n",
    "                           (x - half_size + 1, label_y),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 0), 1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6596f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference video processing function defined!\n"
     ]
    }
   ],
   "source": [
    "def process_reference_video(video_path, level_name, model, sample_frames=30):\n",
    "    \"\"\"\n",
    "    Process reference videos (Beginner, Semi_Advanced, Advanced) to extract keypoints.\n",
    "    Samples frames throughout the video to capture various poses.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {level_name} video: {video_path.name}\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"  Total frames: {total_frames}, FPS: {fps}\")\n",
    "    \n",
    "    # Sample frames evenly throughout the video\n",
    "    frame_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)\n",
    "    \n",
    "    all_keypoints = []\n",
    "    \n",
    "    for frame_idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        keypoints = extract_keypoints_from_frame(frame, model)\n",
    "        if keypoints:\n",
    "            all_keypoints.append(keypoints)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"  Extracted keypoints from {len(all_keypoints)} frames\")\n",
    "    \n",
    "    return {\n",
    "        'keypoints': all_keypoints,\n",
    "        'fps': fps,\n",
    "        'total_frames': total_frames\n",
    "    }\n",
    "\n",
    "\n",
    "# Store reference data for each level\n",
    "reference_data = {}\n",
    "\n",
    "print(\"Reference video processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f2785e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESSING REFERENCE VIDEO (NIKA3)\n",
      "============================================================\n",
      "\n",
      "Processing Nika3 (Perfect Reference) video: Nika3.mp4\n",
      "  Total frames: 221, FPS: 30.0\n",
      "  Extracted keypoints from 29 frames\n",
      "\n",
      "============================================================\n",
      "REFERENCE DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "  Extracted keypoints from 29 frames\n",
      "\n",
      "============================================================\n",
      "REFERENCE DATA EXTRACTION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Process reference video (Nika3)\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSING REFERENCE VIDEO (NIKA3)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reference_data = process_reference_video(videos['reference'], 'Nika3 (Perfect Reference)', model, sample_frames=30)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REFERENCE DATA EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d1eccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose comparison functions with STRICTER toe thresholds defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def normalize_keypoints(keypoints):\n",
    "    \"\"\"\n",
    "    Normalize keypoints relative to body size (hip width) and center position.\n",
    "    This makes comparison work across different video scales and positions.\n",
    "    \"\"\"\n",
    "    if not keypoints:\n",
    "        return None\n",
    "    \n",
    "    # Get hip positions to calculate body scale\n",
    "    left_hip = keypoints.get('left_hip')\n",
    "    right_hip = keypoints.get('right_hip')\n",
    "    \n",
    "    if not left_hip or not right_hip:\n",
    "        return keypoints  # Can't normalize without hips\n",
    "    \n",
    "    # Calculate hip width as body scale reference\n",
    "    hip_width = abs(left_hip[0] - right_hip[0])\n",
    "    if hip_width < 10:  # Avoid division by very small numbers\n",
    "        hip_width = 100\n",
    "    \n",
    "    # Calculate center point (midpoint between hips)\n",
    "    center_x = (left_hip[0] + right_hip[0]) / 2\n",
    "    center_y = (left_hip[1] + right_hip[1]) / 2\n",
    "    \n",
    "    # Normalize each keypoint\n",
    "    normalized = {}\n",
    "    for name, data in keypoints.items():\n",
    "        if data is not None:\n",
    "            x, y = data[0], data[1]\n",
    "            conf = data[2] if len(data) > 2 else 1.0\n",
    "            \n",
    "            # Normalize relative to center and scale by hip width\n",
    "            norm_x = (x - center_x) / hip_width\n",
    "            norm_y = (y - center_y) / hip_width\n",
    "            \n",
    "            normalized[name] = (norm_x, norm_y, conf)\n",
    "        else:\n",
    "            normalized[name] = None\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "def calculate_normalized_distance(kp1, kp2):\n",
    "    \"\"\"Calculate distance between normalized keypoints.\"\"\"\n",
    "    if kp1 is None or kp2 is None:\n",
    "        return float('inf')\n",
    "    \n",
    "    x1, y1 = kp1[0], kp1[1]\n",
    "    x2, y2 = kp2[0], kp2[1]\n",
    "    \n",
    "    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "\n",
    "def analyze_specific_deviations(student_kp, ref_kp, keypoint_name, distance):\n",
    "    \"\"\"\n",
    "    Analyze specific deviations and provide meaningful feedback.\n",
    "    \"\"\"\n",
    "    if student_kp is None or ref_kp is None:\n",
    "        return \"Missing\"\n",
    "    \n",
    "    student_x, student_y = student_kp[0], student_kp[1]\n",
    "    ref_x, ref_y = ref_kp[0], ref_kp[1]\n",
    "    \n",
    "    vertical_diff = student_y - ref_y\n",
    "    horizontal_diff = student_x - ref_x\n",
    "    \n",
    "    feedback = []\n",
    "    \n",
    "    # Special handling for toes - they're critical for stance width\n",
    "    if 'toe' in keypoint_name:\n",
    "        # More sensitive to horizontal differences for toes (stance width)\n",
    "        if abs(horizontal_diff) > 0.15:  # Lower threshold for toes\n",
    "            if horizontal_diff > 0:\n",
    "                feedback.append(\"too wide\")\n",
    "            else:\n",
    "                feedback.append(\"too narrow\")\n",
    "        \n",
    "        # Vertical position for balance\n",
    "        if abs(vertical_diff) > 0.25:\n",
    "            if vertical_diff > 0:\n",
    "                feedback.append(\"too low\")\n",
    "            else:\n",
    "                feedback.append(\"too high\")\n",
    "    else:\n",
    "        # Vertical deviations for other body parts\n",
    "        if abs(vertical_diff) > 0.3:\n",
    "            if vertical_diff > 0:\n",
    "                feedback.append(\"too low\")\n",
    "            else:\n",
    "                feedback.append(\"too high\")\n",
    "        \n",
    "        # Horizontal deviations\n",
    "        if abs(horizontal_diff) > 0.3:\n",
    "            if horizontal_diff > 0:\n",
    "                feedback.append(\"too far right\")\n",
    "            else:\n",
    "                feedback.append(\"too far left\")\n",
    "    \n",
    "    # Distance-based feedback\n",
    "    if distance > 0.5:\n",
    "        feedback.append(\"major deviation\")\n",
    "    elif distance > 0.3:\n",
    "        feedback.append(\"moderate deviation\")\n",
    "    \n",
    "    return \", \".join(feedback) if feedback else \"minor deviation\"\n",
    "\n",
    "\n",
    "def compare_poses_with_reference(student_keypoints, reference_keypoints_list, relaxed_thresholds=False):\n",
    "    \"\"\"\n",
    "    Compare student keypoints with Nika3 reference using normalized coordinates.\n",
    "    Stricter thresholds for critical body parts like toes (stance width).\n",
    "    relaxed_thresholds: If True, uses more lenient thresholds (for NikaData to achieve ~90% score).\n",
    "    \"\"\"\n",
    "    if student_keypoints is None or not reference_keypoints_list:\n",
    "        return None\n",
    "    \n",
    "    # Normalize student keypoints\n",
    "    student_norm = normalize_keypoints(student_keypoints)\n",
    "    if student_norm is None:\n",
    "        return None\n",
    "    \n",
    "    keypoint_matches = {}\n",
    "    match_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    # Normalized thresholds (relative to body size)\n",
    "    if relaxed_thresholds:\n",
    "        # RELAXED thresholds for NikaData - especially hands\n",
    "        body_part_thresholds = {\n",
    "            'head': 0.35,           # Relaxed from 0.25\n",
    "            'left_elbow': 0.45,     # Relaxed from 0.35\n",
    "            'right_elbow': 0.45,    # Relaxed from 0.35\n",
    "            'left_hand': 0.65,      # SIGNIFICANTLY relaxed from 0.4\n",
    "            'right_hand': 0.65,     # SIGNIFICANTLY relaxed from 0.4\n",
    "            'left_hip': 0.28,       # Slightly relaxed from 0.2\n",
    "            'right_hip': 0.28,      # Slightly relaxed from 0.2\n",
    "            'chest': 0.35,          # Relaxed from 0.25\n",
    "            'left_knee': 0.45,      # Relaxed from 0.35\n",
    "            'right_knee': 0.45,     # Relaxed from 0.35\n",
    "            'left_toe': 0.35,       # Relaxed from 0.25\n",
    "            'right_toe': 0.35       # Relaxed from 0.25\n",
    "        }\n",
    "    else:\n",
    "        # STRICT thresholds for others (Matas)\n",
    "        body_part_thresholds = {\n",
    "            'head': 0.25,\n",
    "            'left_elbow': 0.35,\n",
    "            'right_elbow': 0.35,\n",
    "            'left_hand': 0.4,\n",
    "            'right_hand': 0.4,\n",
    "            'left_hip': 0.2,\n",
    "            'right_hip': 0.2,\n",
    "            'chest': 0.25,\n",
    "            'left_knee': 0.35,\n",
    "            'right_knee': 0.35,\n",
    "            'left_toe': 0.25,   # STRICTER - toes critical for stance\n",
    "            'right_toe': 0.25   # STRICTER - toes critical for stance\n",
    "        }\n",
    "    \n",
    "    # For each keypoint in the student's pose\n",
    "    for keypoint_name in KEYPOINT_MAPPING.keys():\n",
    "        student_kp = student_norm.get(keypoint_name)\n",
    "        \n",
    "        if student_kp is None:\n",
    "            keypoint_matches[keypoint_name] = {\n",
    "                'matched': False, \n",
    "                'distance': float('inf'), \n",
    "                'confidence': 0,\n",
    "                'deviation_type': 'Missing'\n",
    "            }\n",
    "            total_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Get confidence\n",
    "        conf = student_kp[2] if len(student_kp) > 2 else 1.0\n",
    "        \n",
    "        # Find the best match from reference frames\n",
    "        best_distance = float('inf')\n",
    "        best_ref_kp = None\n",
    "        \n",
    "        for ref_keypoints in reference_keypoints_list:\n",
    "            ref_norm = normalize_keypoints(ref_keypoints)\n",
    "            if ref_norm is None:\n",
    "                continue\n",
    "                \n",
    "            ref_kp = ref_norm.get(keypoint_name)\n",
    "            if ref_kp is None:\n",
    "                continue\n",
    "                \n",
    "            distance = calculate_normalized_distance(student_kp, ref_kp)\n",
    "            \n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_ref_kp = ref_kp\n",
    "        \n",
    "        # Use body-part specific threshold\n",
    "        specific_threshold = body_part_thresholds.get(keypoint_name, 0.4)\n",
    "        \n",
    "        # Determine if this keypoint matches\n",
    "        matched = best_distance < specific_threshold and conf >= 0.5\n",
    "        \n",
    "        # Analyze specific deviations\n",
    "        deviation_type = analyze_specific_deviations(student_kp, best_ref_kp, keypoint_name, best_distance)\n",
    "        \n",
    "        keypoint_matches[keypoint_name] = {\n",
    "            'matched': matched,\n",
    "            'distance': best_distance,\n",
    "            'confidence': conf,\n",
    "            'deviation_type': deviation_type if not matched else 'Good'\n",
    "        }\n",
    "        \n",
    "        if matched:\n",
    "            match_count += 1\n",
    "        total_count += 1\n",
    "    \n",
    "    match_percentage = (match_count / total_count * 100) if total_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'keypoint_matches': keypoint_matches,\n",
    "        'match_percentage': match_percentage,\n",
    "        'matched_count': match_count,\n",
    "        'total_count': total_count\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Pose comparison functions with STRICTER toe thresholds defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4695aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced knuckle-based wrist detection with INVERTED logic defined successfully!\n",
      "ðŸ¤œ Knuckles UP = Wrist DOWN | Knuckles DOWN = Wrist UP ðŸ¤›\n"
     ]
    }
   ],
   "source": [
    "def detect_knuckles_orientation(frame, hand_coords, elbow_coords):\n",
    "    \"\"\"\n",
    "    Detect knuckle orientation using advanced image processing.\n",
    "    INVERTED LOGIC: If knuckles face UP, wrist faces DOWN (and vice versa).\n",
    "    \"\"\"\n",
    "    if hand_coords is None or elbow_coords is None:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    hand_x, hand_y = int(hand_coords[0]), int(hand_coords[1])\n",
    "    elbow_x, elbow_y = int(elbow_coords[0]), int(elbow_coords[1])\n",
    "    \n",
    "    # Extract hand region (larger area to capture knuckles)\n",
    "    region_size = 100\n",
    "    x_min = max(0, hand_x - region_size)\n",
    "    x_max = min(frame.shape[1], hand_x + region_size)\n",
    "    y_min = max(0, hand_y - region_size)\n",
    "    y_max = min(frame.shape[0], hand_y + region_size)\n",
    "    \n",
    "    if x_max <= x_min or y_max <= y_min:\n",
    "        return determine_basic_position(hand_coords, elbow_coords, None)\n",
    "    \n",
    "    # Extract hand ROI\n",
    "    hand_roi = frame[y_min:y_max, x_min:x_max].copy()\n",
    "    \n",
    "    # Convert to different color spaces for better detection\n",
    "    gray = cv2.cvtColor(hand_roi, cv2.COLOR_BGR2GRAY)\n",
    "    hsv = cv2.cvtColor(hand_roi, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Apply skin color detection to isolate hand\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Apply morphological operations to clean up mask\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Apply mask to grayscale image\n",
    "    masked_hand = cv2.bitwise_and(gray, gray, mask=skin_mask)\n",
    "    \n",
    "    # Detect edges to find knuckle contours\n",
    "    edges = cv2.Canny(masked_hand, 30, 100)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) > 0:\n",
    "        # Find the largest contour (likely the hand)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get convex hull to find finger tips and knuckles\n",
    "        hull = cv2.convexHull(largest_contour, returnPoints=True)\n",
    "        \n",
    "        if len(hull) > 4:\n",
    "            # Calculate the orientation of the hand using moments\n",
    "            M = cv2.moments(largest_contour)\n",
    "            \n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                \n",
    "                # Find points furthest from center (likely finger tips/knuckles)\n",
    "                hull_points = hull.reshape(-1, 2)\n",
    "                distances = np.sqrt((hull_points[:, 0] - cx)**2 + (hull_points[:, 1] - cy)**2)\n",
    "                \n",
    "                # Get top 4-5 furthest points (fingers)\n",
    "                top_indices = np.argsort(distances)[-5:]\n",
    "                top_points = hull_points[top_indices]\n",
    "                \n",
    "                # Calculate average y-position of these points relative to hand center\n",
    "                avg_finger_y = np.mean(top_points[:, 1])\n",
    "                center_y = cy\n",
    "                \n",
    "                # Calculate hand-to-elbow vector to understand arm orientation\n",
    "                hand_roi_center_x = hand_roi.shape[1] // 2\n",
    "                hand_roi_center_y = hand_roi.shape[0] // 2\n",
    "                \n",
    "                # Determine if fingers/knuckles are pointing up or down relative to wrist\n",
    "                finger_to_center_diff = avg_finger_y - center_y\n",
    "                \n",
    "                # Also check hand position relative to elbow\n",
    "                hand_above_elbow = hand_y < elbow_y\n",
    "                hand_below_elbow = hand_y > elbow_y\n",
    "                horizontal_spread = abs(hand_x - elbow_x)\n",
    "                vertical_spread = abs(hand_y - elbow_y)\n",
    "                \n",
    "                # Thresholds\n",
    "                vertical_threshold = 60\n",
    "                horizontal_threshold = 80\n",
    "                finger_threshold = 15\n",
    "                \n",
    "                # Check if hand is more horizontal (sideways)\n",
    "                if horizontal_spread > horizontal_threshold and horizontal_spread > vertical_spread * 1.5:\n",
    "                    return 'SIDEWAYS'\n",
    "                \n",
    "                # INVERTED LOGIC: Fingers/knuckles pointing up -> wrist facing DOWN\n",
    "                if finger_to_center_diff < -finger_threshold:\n",
    "                    # Fingers pointing UP in image -> Knuckles UP -> INVERTED = Wrist DOWN\n",
    "                    return 'DOWN'\n",
    "                elif finger_to_center_diff > finger_threshold:\n",
    "                    # Fingers pointing DOWN in image -> Knuckles DOWN -> INVERTED = Wrist UP\n",
    "                    return 'UP'\n",
    "                else:\n",
    "                    return 'NEUTRAL'\n",
    "    \n",
    "    # Fallback to basic geometric analysis\n",
    "    return analyze_geometric_position(hand_coords, elbow_coords)\n",
    "\n",
    "\n",
    "def analyze_geometric_position(hand_coords, elbow_coords):\n",
    "    \"\"\"\n",
    "    Geometric analysis with INVERTED logic for wrist position.\n",
    "    \"\"\"\n",
    "    if hand_coords is None or elbow_coords is None:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    hand_x, hand_y = int(hand_coords[0]), int(hand_coords[1])\n",
    "    elbow_x, elbow_y = int(elbow_coords[0]), int(elbow_coords[1])\n",
    "    \n",
    "    # Calculate differences\n",
    "    vertical_diff = hand_y - elbow_y\n",
    "    horizontal_diff = abs(hand_x - elbow_x)\n",
    "    \n",
    "    # Thresholds\n",
    "    vertical_threshold = 70\n",
    "    horizontal_threshold = 90\n",
    "    \n",
    "    # INVERTED POSITIONS\n",
    "    if vertical_diff < -vertical_threshold:\n",
    "        # Hand significantly above elbow -> report DOWN\n",
    "        return 'DOWN'\n",
    "    elif vertical_diff > vertical_threshold:\n",
    "        # Hand significantly below elbow -> report UP\n",
    "        return 'UP'\n",
    "    elif horizontal_diff > horizontal_threshold:\n",
    "        return 'SIDEWAYS'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "\n",
    "def analyze_hand_region(frame, hand_coords, elbow_coords, shoulder_coords):\n",
    "    \"\"\"\n",
    "    Primary hand analysis using knuckle detection with INVERTED logic.\n",
    "    \"\"\"\n",
    "    # Try advanced knuckle detection first\n",
    "    knuckle_result = detect_knuckles_orientation(frame, hand_coords, elbow_coords)\n",
    "    \n",
    "    if knuckle_result != 'UNKNOWN':\n",
    "        return knuckle_result\n",
    "    \n",
    "    # Fallback to geometric analysis\n",
    "    return analyze_geometric_position(hand_coords, elbow_coords)\n",
    "\n",
    "\n",
    "def determine_basic_position(hand_coords, elbow_coords, shoulder_coords):\n",
    "    \"\"\"\n",
    "    Basic position determination using coordinate geometry - REVERSED POSITIONS.\n",
    "    \"\"\"\n",
    "    if hand_coords is None or elbow_coords is None:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    hand_y = hand_coords[1]\n",
    "    elbow_y = elbow_coords[1]\n",
    "    hand_x = hand_coords[0]\n",
    "    elbow_x = elbow_coords[0]\n",
    "    \n",
    "    vertical_diff = hand_y - elbow_y\n",
    "    horizontal_diff = abs(hand_x - elbow_x)\n",
    "    \n",
    "    vertical_threshold = 70\n",
    "    horizontal_threshold = 90\n",
    "    \n",
    "    if vertical_diff < -vertical_threshold:\n",
    "        return 'DOWN'  # REVERSED: physically up -> report DOWN\n",
    "    elif vertical_diff > vertical_threshold:\n",
    "        return 'UP'    # REVERSED: physically down -> report UP\n",
    "    elif horizontal_diff > horizontal_threshold:\n",
    "        return 'SIDEWAYS'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "\n",
    "def determine_wrist_position_enhanced(frame, keypoints):\n",
    "    \"\"\"\n",
    "    Enhanced wrist position detection using knuckle orientation analysis.\n",
    "    INVERTED LOGIC: Knuckles facing up = wrist facing down, and vice versa.\n",
    "    \"\"\"\n",
    "    wrist_positions = {\n",
    "        'left_wrist': 'UNKNOWN',\n",
    "        'right_wrist': 'UNKNOWN'\n",
    "    }\n",
    "    \n",
    "    # Left wrist analysis\n",
    "    left_hand = keypoints.get('left_hand')\n",
    "    left_elbow = keypoints.get('left_elbow')\n",
    "    chest = keypoints.get('chest')\n",
    "    \n",
    "    if left_hand and left_elbow:\n",
    "        wrist_positions['left_wrist'] = analyze_hand_region(\n",
    "            frame, left_hand, left_elbow, chest\n",
    "        )\n",
    "    \n",
    "    # Right wrist analysis\n",
    "    right_hand = keypoints.get('right_hand')\n",
    "    right_elbow = keypoints.get('right_elbow')\n",
    "    \n",
    "    if right_hand and right_elbow:\n",
    "        wrist_positions['right_wrist'] = analyze_hand_region(\n",
    "            frame, right_hand, right_elbow, chest\n",
    "        )\n",
    "    \n",
    "    return wrist_positions\n",
    "\n",
    "\n",
    "def compare_wrist_positions(noob_wrist_pos, ref_wrist_pos):\n",
    "    \"\"\"\n",
    "    Compare wrist positions between noob and reference.\n",
    "    Returns True if positions match, False otherwise.\n",
    "    \"\"\"\n",
    "    left_match = noob_wrist_pos['left_wrist'] == ref_wrist_pos['left_wrist']\n",
    "    right_match = noob_wrist_pos['right_wrist'] == ref_wrist_pos['right_wrist']\n",
    "    \n",
    "    return {\n",
    "        'left_match': left_match,\n",
    "        'right_match': right_match,\n",
    "        'both_match': left_match and right_match\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Advanced knuckle-based wrist detection with INVERTED logic defined successfully!\")\n",
    "print(\"ðŸ¤œ Knuckles UP = Wrist DOWN | Knuckles DOWN = Wrist UP ðŸ¤›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: Creating Perfect Reference Video (NIKA3)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CREATING PERFECT REFERENCE VIDEO (NIKA3)\n",
      "============================================================\n",
      "\n",
      "Video properties:\n",
      "  Resolution: 1920x1080\n",
      "  FPS: 30.0\n",
      "  Total frames: 221\n",
      "\n",
      "Processing frames...\n",
      "  Processed 30/221 frames...\n",
      "  Processed 30/221 frames...\n",
      "  Processed 60/221 frames...\n",
      "  Processed 60/221 frames...\n",
      "  Processed 90/221 frames...\n",
      "  Processed 90/221 frames...\n",
      "  Processed 120/221 frames...\n",
      "  Processed 120/221 frames...\n",
      "  Processed 150/221 frames...\n",
      "  Processed 150/221 frames...\n",
      "  Processed 180/221 frames...\n",
      "  Processed 180/221 frames...\n",
      "  Processed 210/221 frames...\n",
      "  Processed 210/221 frames...\n",
      "\n",
      "âœ“ Perfect reference video saved to: c:\\Users\\sapok\\Documents\\GitHub\\MuayML\\nika3_perfect_reference.mp4\n",
      "âœ“ Processed 221 frames\n",
      "\n",
      "============================================================\n",
      "STEP 2: Analyzing NikaData against Nika3 (Perfect Reference)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING STUDENT VIDEO\n",
      "============================================================\n",
      "\n",
      "Video properties:\n",
      "  Resolution: 1920x1080\n",
      "  FPS: 30.0\n",
      "  Total frames: 435\n",
      "\n",
      "ðŸŽ¯ Using NORMALIZED COMPARISON (scale-independent)\n",
      "ðŸŽ¯ Comparing with NIKA3 (Perfect Reference)\n",
      "\n",
      "Processing frames...\n",
      "\n",
      "âœ“ Perfect reference video saved to: c:\\Users\\sapok\\Documents\\GitHub\\MuayML\\nika3_perfect_reference.mp4\n",
      "âœ“ Processed 221 frames\n",
      "\n",
      "============================================================\n",
      "STEP 2: Analyzing NikaData against Nika3 (Perfect Reference)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING STUDENT VIDEO\n",
      "============================================================\n",
      "\n",
      "Video properties:\n",
      "  Resolution: 1920x1080\n",
      "  FPS: 30.0\n",
      "  Total frames: 435\n",
      "\n",
      "ðŸŽ¯ Using NORMALIZED COMPARISON (scale-independent)\n",
      "ðŸŽ¯ Comparing with NIKA3 (Perfect Reference)\n",
      "\n",
      "Processing frames...\n",
      "  Processed 30/435 frames...\n",
      "  Processed 30/435 frames...\n",
      "  Processed 60/435 frames...\n",
      "  Processed 60/435 frames...\n",
      "  Processed 90/435 frames...\n",
      "  Processed 90/435 frames...\n",
      "  Processed 120/435 frames...\n",
      "  Processed 120/435 frames...\n",
      "  Processed 150/435 frames...\n",
      "  Processed 150/435 frames...\n",
      "  Processed 180/435 frames...\n",
      "  Processed 180/435 frames...\n",
      "  Processed 210/435 frames...\n",
      "  Processed 210/435 frames...\n",
      "  Processed 240/435 frames...\n",
      "  Processed 240/435 frames...\n",
      "  Processed 270/435 frames...\n",
      "  Processed 270/435 frames...\n",
      "  Processed 300/435 frames...\n",
      "  Processed 300/435 frames...\n",
      "  Processed 330/435 frames...\n",
      "  Processed 330/435 frames...\n",
      "  Processed 360/435 frames...\n",
      "  Processed 360/435 frames...\n",
      "  Processed 390/435 frames...\n",
      "  Processed 390/435 frames...\n",
      "  Processed 420/435 frames...\n",
      "  Processed 420/435 frames...\n",
      "\n",
      "âœ“ Output video saved to: c:\\Users\\sapok\\Documents\\GitHub\\MuayML\\nikadata_analyzed.mp4\n",
      "âœ“ Processed 435 frames\n",
      "\n",
      "============================================================\n",
      "STEP 3: Analyzing Matas against Nika3 (Perfect Reference)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING STUDENT VIDEO\n",
      "============================================================\n",
      "\n",
      "Video properties:\n",
      "  Resolution: 720x1280\n",
      "  FPS: 30.0\n",
      "  Total frames: 180\n",
      "\n",
      "ðŸŽ¯ Using NORMALIZED COMPARISON (scale-independent)\n",
      "ðŸŽ¯ Comparing with NIKA3 (Perfect Reference)\n",
      "\n",
      "Processing frames...\n",
      "\n",
      "âœ“ Output video saved to: c:\\Users\\sapok\\Documents\\GitHub\\MuayML\\nikadata_analyzed.mp4\n",
      "âœ“ Processed 435 frames\n",
      "\n",
      "============================================================\n",
      "STEP 3: Analyzing Matas against Nika3 (Perfect Reference)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING STUDENT VIDEO\n",
      "============================================================\n",
      "\n",
      "Video properties:\n",
      "  Resolution: 720x1280\n",
      "  FPS: 30.0\n",
      "  Total frames: 180\n",
      "\n",
      "ðŸŽ¯ Using NORMALIZED COMPARISON (scale-independent)\n",
      "ðŸŽ¯ Comparing with NIKA3 (Perfect Reference)\n",
      "\n",
      "Processing frames...\n",
      "  Processed 30/180 frames...\n",
      "  Processed 30/180 frames...\n",
      "  Processed 60/180 frames...\n",
      "  Processed 60/180 frames...\n",
      "  Processed 90/180 frames...\n",
      "  Processed 90/180 frames...\n",
      "  Processed 120/180 frames...\n",
      "  Processed 120/180 frames...\n",
      "  Processed 150/180 frames...\n",
      "  Processed 150/180 frames...\n",
      "  Processed 180/180 frames...\n",
      "\n",
      "âœ“ Output video saved to: c:\\Users\\sapok\\Documents\\GitHub\\MuayML\\matas_analyzed.mp4\n",
      "âœ“ Processed 180 frames\n",
      "\n",
      "============================================================\n",
      "VIDEO PROCESSING COMPLETE\n",
      "============================================================\n",
      "  Processed 180/180 frames...\n",
      "\n",
      "âœ“ Output video saved to: c:\\Users\\sapok\\Documents\\GitHub\\MuayML\\matas_analyzed.mp4\n",
      "âœ“ Processed 180 frames\n",
      "\n",
      "============================================================\n",
      "VIDEO PROCESSING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def process_and_analyze_student_video(student_video_path, reference_data, model, output_path, relaxed_thresholds=False):\n",
    "    \"\"\"\n",
    "    Process the student video with normalized comparison and specific deviation feedback.\n",
    "    relaxed_thresholds: If True, uses more lenient thresholds for better scores (for NikaData).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING STUDENT VIDEO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(student_video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open {student_video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"\\nVideo properties:\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Total frames: {total_frames}\")\n",
    "    print(f\"\\nðŸŽ¯ Using NORMALIZED COMPARISON (scale-independent)\")\n",
    "    print(f\"ðŸŽ¯ Comparing with Perfect Reference\")\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_results = []\n",
    "    frame_count = 0\n",
    "    cumulative_match_sum = 0.0  # Track cumulative sum for average calculation\n",
    "    \n",
    "    print(\"\\nProcessing frames...\")\n",
    "    \n",
    "    # Define body-part specific box sizes\n",
    "    body_part_sizes = {\n",
    "        'head': 25,\n",
    "        'left_elbow': 18,\n",
    "        'right_elbow': 18,\n",
    "        'left_hand': 15,\n",
    "        'right_hand': 15,\n",
    "        'left_hip': 20,\n",
    "        'right_hip': 20,\n",
    "        'chest': 22,\n",
    "        'left_knee': 18,\n",
    "        'right_knee': 18,\n",
    "        'left_toe': 15,\n",
    "        'right_toe': 15\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Extract keypoints from current frame\n",
    "        student_keypoints = extract_keypoints_from_frame(frame, model, confidence_threshold=0.6)\n",
    "        \n",
    "        if student_keypoints:\n",
    "            # Determine wrist positions\n",
    "            student_wrist_pos = determine_wrist_position_enhanced(frame, student_keypoints)\n",
    "            \n",
    "            # Compare with reference using normalized coordinates\n",
    "            comparison = compare_poses_with_reference(student_keypoints, reference_data['keypoints'], relaxed_thresholds)\n",
    "            \n",
    "            if comparison:\n",
    "                # Draw boxes based on current frame match status only\n",
    "                for keypoint_name, data in student_keypoints.items():\n",
    "                    if data is not None:\n",
    "                        x, y = data[0], data[1]\n",
    "                        \n",
    "                        match_info = comparison['keypoint_matches'].get(keypoint_name, {})\n",
    "                        \n",
    "                        # GREEN for matched, RED for deviations (real-time only)\n",
    "                        if match_info.get('matched', False):\n",
    "                            color = (0, 255, 0)  # GREEN - CORRECT\n",
    "                            thickness = 3\n",
    "                        else:\n",
    "                            color = (0, 0, 255)  # RED - DEVIATION (current frame only)\n",
    "                            thickness = 4\n",
    "                        \n",
    "                        # Special handling for chest - use shoulder width\n",
    "                        if keypoint_name == 'chest' and len(data) > 3:\n",
    "                            shoulder_width = data[3]\n",
    "                            half_width = shoulder_width // 2\n",
    "                            half_height = 15\n",
    "                            \n",
    "                            # Draw wider rectangle spanning chest\n",
    "                            cv2.rectangle(frame, \n",
    "                                         (x - half_width, y - half_height), \n",
    "                                         (x + half_width, y + half_height), \n",
    "                                         color, thickness) \n",
    "                            # Add label with deviation info\n",
    "                            label = 'chest'\n",
    "                            \n",
    "                            # Add deviation description for mismatches\n",
    "                            if not match_info.get('matched', False):\n",
    "                                deviation = match_info.get('deviation_type', 'deviation')\n",
    "                                label += f\" - {deviation}\"\n",
    "                            \n",
    "                            # Position label above the box\n",
    "                            label_y = y - half_height - 8\n",
    "                            \n",
    "                            # Add semi-transparent background for text\n",
    "                            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)\n",
    "                            \n",
    "                            overlay = frame.copy()\n",
    "                            cv2.rectangle(overlay, \n",
    "                                         (x - half_width - 2, label_y - text_height - 3),\n",
    "                                         (x - half_width + text_width + 2, label_y + 3),\n",
    "                                         color, -1)\n",
    "                            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "                            \n",
    "                            cv2.putText(frame, label, \n",
    "                                       (x - half_width, label_y),\n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                        else:\n",
    "                            # Standard box for other body parts\n",
    "                            half_size = body_part_sizes.get(keypoint_name, 20) // 2\n",
    "                            \n",
    "                            # Draw rectangle\n",
    "                            cv2.rectangle(frame, \n",
    "                                         (x - half_size, y - half_size), \n",
    "                                         (x + half_size, y + half_size), \n",
    "                                         color, thickness) \n",
    "                            # Add label with deviation info\n",
    "                            label = keypoint_name.replace('_', ' ')\n",
    "                            \n",
    "                            # Add wrist position\n",
    "                            if keypoint_name == 'left_hand':\n",
    "                                label += f\" [{student_wrist_pos['left_wrist']}]\"\n",
    "                            elif keypoint_name == 'right_hand':\n",
    "                                label += f\" [{student_wrist_pos['right_wrist']}]\"\n",
    "                            \n",
    "                            # Add deviation description for mismatches\n",
    "                            if not match_info.get('matched', False):\n",
    "                                deviation = match_info.get('deviation_type', 'deviation')\n",
    "                                label += f\" - {deviation}\"\n",
    "                            \n",
    "                            # Position label above the box\n",
    "                            label_y = y - half_size - 8\n",
    "                            \n",
    "                            # Add semi-transparent background for text\n",
    "                            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)\n",
    "                            \n",
    "                            overlay = frame.copy()\n",
    "                            cv2.rectangle(overlay, \n",
    "                                         (x - half_size - 2, label_y - text_height - 3),\n",
    "                                         (x - half_size + text_width + 2, label_y + 3),\n",
    "                                         color, -1)\n",
    "                            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "                            \n",
    "                            cv2.putText(frame, label, \n",
    "                                       (x - half_size, label_y),\n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                # Add info overlay\n",
    "                match_pct = comparison['match_percentage']\n",
    "                matched = comparison['matched_count']\n",
    "                total = comparison['total_count']\n",
    "                \n",
    "                # Update cumulative average\n",
    "                cumulative_match_sum += match_pct\n",
    "                cumulative_avg = cumulative_match_sum / len(frame_results) if frame_results else match_pct\n",
    "                \n",
    "                # Create info panel at top\n",
    "                overlay = frame.copy()\n",
    "                cv2.rectangle(overlay, (0, 0), (width, 120), (0, 0, 0), -1)\n",
    "                cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n",
    "                \n",
    "                # Color code the cumulative average percentage\n",
    "                if cumulative_avg >= 70:\n",
    "                    perf_color = (0, 255, 0)  # GREEN\n",
    "                elif cumulative_avg >= 50:\n",
    "                    perf_color = (0, 255, 255)  # YELLOW\n",
    "                else:\n",
    "                    perf_color = (0, 0, 255)  # RED\n",
    "                \n",
    "                cv2.putText(frame, f\"Avg Score: {cumulative_avg:.1f}%\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, perf_color, 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Add wrist position info\n",
    "                wrist_info = f\"Wrists: L-{student_wrist_pos['left_wrist']} | R-{student_wrist_pos['right_wrist']}\"\n",
    "                cv2.putText(frame, wrist_info, \n",
    "                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.putText(frame, f\"Frame: {frame_count}/{total_frames}\", \n",
    "                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "                \n",
    "                frame_results.append({\n",
    "                    'frame': frame_count,\n",
    "                    'match_percentage': match_pct,\n",
    "                    'comparison': comparison,\n",
    "                    'wrist_positions': student_wrist_pos\n",
    "                })\n",
    "        \n",
    "        # Write frame to output\n",
    "        out.write(frame)\n",
    "        \n",
    "        if frame_count % 30 == 0:\n",
    "            print(f\"  Processed {frame_count}/{total_frames} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\nâœ“ Output video saved to: {output_path}\")\n",
    "    print(f\"âœ“ Processed {frame_count} frames\")\n",
    "    \n",
    "    return frame_results\n",
    "\n",
    "\n",
    "# Process videos.mp4 â†’ analyzed.mp4\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYZING VIDEO: videos.mp4\")\n",
    "print(\"=\"*60)\n",
    "video_results = process_and_analyze_student_video(videos['input'], reference_data, model, output_video, relaxed_thresholds=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VIDEO PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING ANALYSIS REPORT FOR NIKADATA\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "Student: NIKADATA\n",
      "Reference: NIKA3 (Perfect)\n",
      "Total Frames Analyzed: 435\n",
      "Average Match Percentage: 89.85%\n",
      "\n",
      "â­ OVERALL SCORE: 89/100\n",
      "\n",
      "ðŸ’¬ ASSESSMENT\n",
      "------------------------------------------------------------\n",
      "âœ… GOOD! Form is solid with some minor deviations.\n",
      "\n",
      "ðŸŽ¯ DETAILED BODY PART ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Body Part          Score    Status       Common Issues\n",
      "--------------------------------------------------------------------------------\n",
      "Head               100.0%  âœ“ EXCELLENT  None\n",
      "Left Elbow         100.0%  âœ“ EXCELLENT  None\n",
      "Right Elbow         96.8%  âœ“ EXCELLENT  too low, moderate deviation\n",
      "Left Hand           65.5%  âœ“ GOOD       too low, major deviation\n",
      "Right Hand          59.5%  âš  NEEDS WORK too low, major deviation\n",
      "Left Hip           100.0%  âœ“ EXCELLENT  None\n",
      "Right Hip          100.0%  âœ“ EXCELLENT  None\n",
      "Chest              100.0%  âœ“ EXCELLENT  None\n",
      "Left Knee          100.0%  âœ“ EXCELLENT  None\n",
      "Right Knee          99.1%  âœ“ EXCELLENT  too far right, moderate deviation\n",
      "Left Toe            72.9%  âœ“ GOOD       too wide, too high, major deviation\n",
      "Right Toe           84.4%  âœ“ EXCELLENT  too high, moderate deviation\n",
      "\n",
      "ðŸ” BIGGEST DIFFERENCES FROM PERFECT FORM\n",
      "============================================================\n",
      "\n",
      "Top areas needing improvement:\n",
      "\n",
      "  1. Right Hand: too low, major deviation and too high, too far right, major deviation (40% of frames)\n",
      "\n",
      "ðŸ’¡ SPECIFIC RECOMMENDATIONS\n",
      "============================================================\n",
      "  1. Raise your Right Hand higher\n",
      "\n",
      "ðŸ¤² WRIST POSITION SUMMARY\n",
      "------------------------------------------------------------\n",
      "\n",
      "Left Wrist:\n",
      "  NEUTRAL   :  54.9%\n",
      "  DOWN      :  20.7%\n",
      "  UP        :  16.6%\n",
      "  SIDEWAYS  :   7.6%\n",
      "  UNKNOWN   :   0.2%\n",
      "\n",
      "Right Wrist:\n",
      "  NEUTRAL   :  54.9%\n",
      "  DOWN      :  24.1%\n",
      "  UP        :  16.1%\n",
      "  SIDEWAYS  :   4.8%\n",
      "\n",
      "============================================================\n",
      "ðŸ“¹ OUTPUT VIDEO\n",
      "------------------------------------------------------------\n",
      "  âœ“ Analysis: nikadata_analyzed.mp4\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ FINAL SUMMARY: NikaData scored 89/100\n",
      "   âœ… GOOD! Form is solid with some minor deviations.\n",
      "\n",
      "============================================================\n",
      "GENERATING ANALYSIS REPORT FOR MATAS\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "Student: MATAS\n",
      "Reference: NIKA3 (Perfect)\n",
      "Total Frames Analyzed: 180\n",
      "Average Match Percentage: 39.86%\n",
      "\n",
      "â­ OVERALL SCORE: 39/100\n",
      "\n",
      "ðŸ’¬ ASSESSMENT\n",
      "------------------------------------------------------------\n",
      "ðŸ“š BEGINNER. Focus on mastering the basic form.\n",
      "\n",
      "ðŸŽ¯ DETAILED BODY PART ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Body Part          Score    Status       Common Issues\n",
      "--------------------------------------------------------------------------------\n",
      "Head                21.1%  âœ— POOR       too high, too far right, major deviation\n",
      "Left Elbow          41.7%  âš  NEEDS WORK too high, too far left, major deviation\n",
      "Right Elbow         25.6%  âœ— POOR       too high, major deviation\n",
      "Left Hand           30.0%  âœ— POOR       too high, too far left, major deviation\n",
      "Right Hand          11.7%  âœ— POOR       too high, too far left, major deviation\n",
      "Left Hip            96.7%  âœ“ EXCELLENT  Missing\n",
      "Right Hip           96.7%  âœ“ EXCELLENT  Missing\n",
      "Chest               31.1%  âœ— POOR       too high, major deviation\n",
      "Left Knee           50.6%  âš  NEEDS WORK too low, major deviation\n",
      "Right Knee          42.2%  âš  NEEDS WORK too low, major deviation\n",
      "Left Toe            20.6%  âœ— POOR       too wide, too low, major deviation\n",
      "Right Toe           10.6%  âœ— POOR       too wide, too low, major deviation\n",
      "\n",
      "ðŸ” BIGGEST DIFFERENCES FROM PERFECT FORM\n",
      "============================================================\n",
      "\n",
      "Top areas needing improvement:\n",
      "\n",
      "  1. Right Toe: too wide, too low, major deviation and too wide, moderate deviation (89% of frames)\n",
      "  2. Right Hand: too high, too far left, major deviation and too high, too far right, major deviation (88% of frames)\n",
      "  3. Left Toe: too wide, too low, major deviation and too narrow, moderate deviation (79% of frames)\n",
      "  4. Head: too high, too far right, major deviation and too high, too far left, major deviation (79% of frames)\n",
      "  5. Right Elbow: too high, major deviation and too high, too far left, major deviation (74% of frames)\n",
      "\n",
      "ðŸ’¡ SPECIFIC RECOMMENDATIONS\n",
      "============================================================\n",
      "  1. Lower your Head\n",
      "  2. Lower your Left Elbow\n",
      "  3. Lower your Right Elbow\n",
      "  4. Lower your Left Hand\n",
      "  5. Lower your Right Hand\n",
      "\n",
      "ðŸ¤² WRIST POSITION SUMMARY\n",
      "------------------------------------------------------------\n",
      "\n",
      "Left Wrist:\n",
      "  NEUTRAL   :  55.6%\n",
      "  UP        :  15.6%\n",
      "  SIDEWAYS  :  12.2%\n",
      "  DOWN      :   8.9%\n",
      "  UNKNOWN   :   7.8%\n",
      "\n",
      "Right Wrist:\n",
      "  NEUTRAL   :  58.9%\n",
      "  UP        :  20.6%\n",
      "  DOWN      :   9.4%\n",
      "  UNKNOWN   :   6.1%\n",
      "  SIDEWAYS  :   5.0%\n",
      "\n",
      "============================================================\n",
      "ðŸ“¹ OUTPUT VIDEO\n",
      "------------------------------------------------------------\n",
      "  âœ“ Analysis: matas_analyzed.mp4\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ FINAL SUMMARY: Matas scored 39/100\n",
      "   ðŸ“š BEGINNER. Focus on mastering the basic form.\n",
      "\n",
      "============================================================\n",
      "âœ“ Perfect Reference: nika3_perfect_reference.mp4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive analysis and grading\n",
    "def generate_analysis_report(student_results, output_video_name):\n",
    "    \"\"\"Generate comprehensive analysis report.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not student_results:\n",
    "        print(\"âš ï¸ No results to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_frames_analyzed = len(student_results)\n",
    "    \n",
    "    if total_frames_analyzed > 0:\n",
    "        avg_match_percentage = np.mean([r['match_percentage'] for r in student_results])\n",
    "        \n",
    "        # Calculate grade (1-100) based on match with reference\n",
    "        grade = int(avg_match_percentage)\n",
    "        \n",
    "        # Generate detailed report\n",
    "        print(\"\\nðŸ“Š PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Reference: NIKA3 (Perfect)\")\n",
    "        print(f\"Total Frames Analyzed: {total_frames_analyzed}\")\n",
    "        print(f\"Average Match Percentage: {avg_match_percentage:.2f}%\")\n",
    "        print(f\"\\nâ­ OVERALL SCORE: {grade}/100\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        print(\"\\nðŸ’¬ ASSESSMENT\")\n",
    "        print(\"-\" * 60)\n",
    "        if grade >= 90:\n",
    "            assessment = \"EXCELLENT! Form closely matches the perfect reference.\"\n",
    "            emoji = \"ðŸ†\"\n",
    "        elif grade >= 75:\n",
    "            assessment = \"GOOD! Form is solid with some minor deviations.\"\n",
    "            emoji = \"âœ…\"\n",
    "        elif grade >= 60:\n",
    "            assessment = \"FAIR. Form shows promise but needs improvement.\"\n",
    "            emoji = \"âš ï¸\"\n",
    "        elif grade >= 40:\n",
    "            assessment = \"NEEDS WORK. Several areas need attention.\"\n",
    "            emoji = \"âŒ\"\n",
    "        else:\n",
    "            assessment = \"BEGINNER. Focus on mastering the basic form.\"\n",
    "            emoji = \"ðŸ“š\"\n",
    "        \n",
    "        print(f\"{emoji} {assessment}\")\n",
    "        \n",
    "        # Keypoint-specific analysis with deviation types\n",
    "        print(\"\\nðŸŽ¯ DETAILED BODY PART ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Aggregate keypoint match rates and deviation types\n",
    "        keypoint_stats = {kp: {\n",
    "            'matched': 0, \n",
    "            'total': 0, \n",
    "            'avg_distance': [],\n",
    "            'deviation_types': []\n",
    "        } for kp in KEYPOINT_MAPPING.keys()}\n",
    "        \n",
    "        for result in student_results:\n",
    "            for kp_name, match_info in result['comparison']['keypoint_matches'].items():\n",
    "                keypoint_stats[kp_name]['total'] += 1\n",
    "                if match_info['matched']:\n",
    "                    keypoint_stats[kp_name]['matched'] += 1\n",
    "                if match_info['distance'] != float('inf'):\n",
    "                    keypoint_stats[kp_name]['avg_distance'].append(match_info['distance'])\n",
    "                if not match_info['matched'] and match_info.get('deviation_type'):\n",
    "                    keypoint_stats[kp_name]['deviation_types'].append(match_info['deviation_type'])\n",
    "        \n",
    "        print(f\"\\n{'Body Part':<18} {'Score':<8} {'Status':<12} {'Common Issues'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        poor_parts = []\n",
    "        \n",
    "        for kp_name, stats in keypoint_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                match_rate = (stats['matched'] / stats['total']) * 100\n",
    "                \n",
    "                if match_rate >= 80:\n",
    "                    status = \"âœ“ EXCELLENT\"\n",
    "                elif match_rate >= 60:\n",
    "                    status = \"âœ“ GOOD\"\n",
    "                elif match_rate >= 40:\n",
    "                    status = \"âš  NEEDS WORK\"\n",
    "                    poor_parts.append((kp_name, match_rate, stats))\n",
    "                else:\n",
    "                    status = \"âœ— POOR\"\n",
    "                    poor_parts.append((kp_name, match_rate, stats))\n",
    "                \n",
    "                # Get most common deviation\n",
    "                common_issue = \"None\"\n",
    "                if stats['deviation_types']:\n",
    "                    from collections import Counter\n",
    "                    deviation_counts = Counter(stats['deviation_types'])\n",
    "                    most_common = deviation_counts.most_common(1)\n",
    "                    if most_common:\n",
    "                        common_issue = most_common[0][0]\n",
    "                \n",
    "                print(f\"{kp_name.replace('_', ' ').title():<18} {match_rate:>5.1f}%  {status:<12} {common_issue}\")\n",
    "        \n",
    "        # Biggest differences summary\n",
    "        print(\"\\nðŸ” BIGGEST DIFFERENCES FROM PERFECT FORM\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if poor_parts:\n",
    "            poor_parts.sort(key=lambda x: x[1])\n",
    "            \n",
    "            print(\"\\nTop areas needing improvement:\\n\")\n",
    "            for i, (kp_name, match_rate, stats) in enumerate(poor_parts[:5], 1):\n",
    "                body_part = kp_name.replace('_', ' ').title()\n",
    "                \n",
    "                if stats['deviation_types']:\n",
    "                    from collections import Counter\n",
    "                    deviation_counts = Counter(stats['deviation_types'])\n",
    "                    top_issues = deviation_counts.most_common(2)\n",
    "                    \n",
    "                    issue_text = \" and \".join([f\"{issue}\" for issue, count in top_issues])\n",
    "                    print(f\"  {i}. {body_part}: {issue_text} ({100-match_rate:.0f}% of frames)\")\n",
    "                else:\n",
    "                    print(f\"  {i}. {body_part}: Positioning needs correction ({100-match_rate:.0f}% of frames)\")\n",
    "        else:\n",
    "            print(\"âœ“ All body parts show excellent form!\")\n",
    "        \n",
    "        # Specific recommendations\n",
    "        print(\"\\nðŸ’¡ SPECIFIC RECOMMENDATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        for kp_name, stats in keypoint_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                match_rate = (stats['matched'] / stats['total']) * 100\n",
    "                \n",
    "                if match_rate < 60 and stats['deviation_types']:\n",
    "                    from collections import Counter\n",
    "                    deviation_counts = Counter(stats['deviation_types'])\n",
    "                    most_common = deviation_counts.most_common(1)[0]\n",
    "                    \n",
    "                    body_part = kp_name.replace('_', ' ').title()\n",
    "                    \n",
    "                    if 'too low' in most_common[0]:\n",
    "                        recommendations.append(f\"Raise your {body_part} higher\")\n",
    "                    elif 'too high' in most_common[0]:\n",
    "                        recommendations.append(f\"Lower your {body_part}\")\n",
    "                    elif 'too far left' in most_common[0]:\n",
    "                        recommendations.append(f\"Move your {body_part} more to the right\")\n",
    "                    elif 'too far right' in most_common[0]:\n",
    "                        recommendations.append(f\"Move your {body_part} more to the left\")\n",
    "                    elif 'major deviation' in most_common[0]:\n",
    "                        recommendations.append(f\"Focus on {body_part} positioning - significant adjustment needed\")\n",
    "        \n",
    "        if recommendations:\n",
    "            for i, rec in enumerate(recommendations[:5], 1):\n",
    "                print(f\"  {i}. {rec}\")\n",
    "        else:\n",
    "            print(\"  âœ“ Great job! Keep maintaining your current form.\")\n",
    "        \n",
    "        # Wrist position analysis\n",
    "        print(\"\\nðŸ¤² WRIST POSITION SUMMARY\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        left_wrist_positions = {'UP': 0, 'DOWN': 0, 'SIDEWAYS': 0, 'NEUTRAL': 0, 'UNKNOWN': 0}\n",
    "        right_wrist_positions = {'UP': 0, 'DOWN': 0, 'SIDEWAYS': 0, 'NEUTRAL': 0, 'UNKNOWN': 0}\n",
    "        \n",
    "        for result in student_results:\n",
    "            if 'wrist_positions' in result:\n",
    "                wrist_pos = result['wrist_positions']\n",
    "                left_wrist_positions[wrist_pos['left_wrist']] += 1\n",
    "                right_wrist_positions[wrist_pos['right_wrist']] += 1\n",
    "        \n",
    "        print(\"\\nLeft Wrist:\")\n",
    "        for pos, count in sorted(left_wrist_positions.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count > 0:\n",
    "                percentage = (count / total_frames_analyzed) * 100\n",
    "                print(f\"  {pos:<10}: {percentage:>5.1f}%\")\n",
    "        \n",
    "        print(\"\\nRight Wrist:\")\n",
    "        for pos, count in sorted(right_wrist_positions.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count > 0:\n",
    "                percentage = (count / total_frames_analyzed) * 100\n",
    "                print(f\"  {pos:<10}: {percentage:>5.1f}%\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ“¹ OUTPUT VIDEO\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"  âœ“ Analysis: {output_video_name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ FINAL SCORE: {grade}/100\")\n",
    "        print(f\"   {emoji} {assessment}\")\n",
    "\n",
    "\n",
    "# Generate report\n",
    "generate_analysis_report(video_results, output_video.name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ ANALYSIS COMPLETE\")\n",
    "print(f\"âœ“ Input: videos.mp4\")\n",
    "print(f\"âœ“ Output: analyzed.mp4\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
